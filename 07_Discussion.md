## Discussion

In light of a collection of results across task domains distinguishing between the performance of prototype- and instance-based models of memory-based behavior, we searched for evidence of a similar distinction with respect to the free recall task paradigm. To do this, we specified and compared the original prototype-based implementation of an established model of task performance based in retrieved context theory (PrototypeCMR) against a parallel instance-based variant (CMR) across diverse experimental conditions, including a dataset featuring variable study list lengths across trials and a dataset featuring item repetitions within trials and variable item repetition spacing between trials. While our simulation analyses focused on the effect of item repetition on recall rates identified some hypothetical distinctions between model predictions, the model variants accounted for human performance on the free recall task with similar effectiveness in each dataset considered.

One clear conclusion we can draw from these analyses is that the success of the theoretical commitments made by the Context Maintenance and Retrieval model and models like it are likely not especially dependent on the architectures in which they are implemented. Instead, the main insights of retrieved context theories (at least as formalized by CMR) are highly portable, and can likely be esconced within any reasonable model architecture where memory search via temporal contextual representations might prove valuable. Establishing the portability of these successful theoretical principles across modeling approaches helps advance the historical effort in cognitive science to develop "a general account of memory and its processes in a working computational system to produce a common explanation of behavior rather than a set of lab-specific and domain-specific theories for different behaviors" [@newell1973you; @jamieson2018instance].

This finding has been increasingly validated lately in other work. @logan2021serial for example similarly embeds mechanisms for maintaining and organizing retrieval using temporal contextual representations within an instance-based architecture to simultaneously account for performance on substantively distinct variations of a task requiring participants to encode and report random strings in left-to-right order by typing them on a computer keyboard, including whole report, serial recall, and copy typing. Other projects more motivated by neuroscientific data (e.g. [@ketz2013theta]; [@schapiro2017complementary]) embed mechanisms for context-based retrieval within detailed formal accounts of hippocampus functionality more complex than either the instance-based or hebbian associative network architectures considered in this work. Our head-to-head comparison of this instance-based account of context maintenance and retrieval against its more standard prototype-based counterpart and observation that both competitively explain free recall performance under varied conditions further evinces the architectural independence of the retrieved context account of memory search.

How do these results fit into the context of other work identifying substantive contrasts between instance- and prototype-based models? Research by @jamieson2018instance comparing the model architectures' capacity to account for semantic memory emphasizes that the main limitation of prototype-based models is the information that they discard or distort toward some center-of-tendency at encoding - idiosyncratic item or contextual features that do not reflect generalities across experience. With this information discarded or suppressed, memory cues selective for those idiosyncratic features cannot result in retrieval of relevant information. Instance-based models on the other hand are able to select information across learning episodes to include in an abstractive representation based on the content of a cue, enabling flexible retrieval of idiosyncratic features while suppressing more common but irrelevant features.

The trace-based application of instance models' $\tau$ parameter is described as fundamental to the unique flexibility of instance-based models outlined by @jamieson2018instance, as it enables instance-based models to modulate the influence of particular memory traces in a retrieved echo representation nonlinearly based on the traces' similarity to a probe. However, while the prototype-based semantic memory models examined by @jamieson2018instance exclude a similar response scaling mechanism, the standard prototype-based specification of CMR does include one. Research on category learning [@nosofsky2002exemplar; @stanton2002comparisons] also contrasting prototype- and instance-based models of the behavior also identifies instance models' characteristic response-scaling mechanism as crucial for accounting for deterministic patterns in memory performance under various research conditions. However, they also evaluate prototype-based models that, like CMR, do include response-scaling mechanisms -- though by definition only instance-based models apply the mechanism to similarities computed between traces and probe representations. To compare the instance-based Exemplar-Generalization model against a prototype-based model with a similar response scaling mechanism, @nosofsky2002exemplar focused on how the models differentially characterize generalization, in this case the category assignment of novel items excluded from initial training. Finding that participants mroe often classify items in the the same categories based on their similarity to one another rather than based on similarity to hypothetical prototype-representations, the instance-based Exemplar-Generalization model came out ahead.

Even the research above drawing distinctions between the explanatory performance of instance-based and prototype-based models report experimental conditions where the two architectures perform similarly. We can conclude that either the considered research conditions or the model specifications themselves also sidestep any of their more substantive differences. Two assumptions enforced in both the prototype- and instance-based frameworks compared here as well as in corresponding datasets were that list item were effectively representationally orthogonal, and encountered just once or twice before retrieval. Furthermore, contextual states as characterized by CMR differ a consistent amount from item to item during study in a traditional list learning experiment. The assumptions together may prevent a distinction from emerging between highly common and highly idiosyncratic item or contextual features under traditional research conditions as emphasized in architectural comparisons drawn by @jamieson2018instance. Similarly, the uniform similarity structure of list items studied and recalled across evaluated datasets here potentially sidesteps issues raised by @nosofsky2002exemplar with prototype-based models.

Higher rates of item repetition or enforced distortions of contextual variation (such as by dividing an encoding phase into distinct trials or sessions) might be enough to more clearly distinguish architecture performance. Simulations of high rates of item repetitions reported in [Figure @fig-repeffect] identify one potentially relevant difference between InstanceCMR and PrototypeCMR -- an exponential rate of increase of recall rates for repeated items in the former, but not the latter -- but the distinction seems independent from contrasts drawn between the architectures drawn by other researchers such as @jamieson2018instance and @nosofsky2002exemplar. By contrast, research conditions where items are repeated rarely in some contexts but frequently in others or nonorthogonal item features influence and are factored into model performance would further explore the relevance of prior exploration of prototype- and instance-based architectures to our understanding of free recall and similar tasks. At the same time, our current results establish that architectural distinctions relevant in other tasks domains may be not particularly critical for accounting for performance across the more traditional research conditions explored here.

While these results suggest some level of equivalance between instance-based and prototype-based models with respect to accounting for free recall performance, their generality is as limited to the simple architectures evaluated as to the datasets explored. More complex or just different models that might be classed in one of these categories may not exhibit the same patterns. For example, the examinations here only consider a constrained conceptualization of instance-based models styled after the MINERVA 2 simulation model of human memory [@hintzman1984minerva]. @lehman2013buffer produced a dual store model of performance on various recall tasks and can be classed as an instance-based model despite excluding some traditional features of models inspired by @hintzman1984minerva, such as reliance on a single trace store and application of a nonlinear response scaling mechanism during retrieval. Its main assumption is that a limited-capacity buffer tracks both information about items and about associations between items and between items and their encoding context; at the same time, it supposes that a secondary, unlimited-capacity store is, with some probability, populated with traces from this buffer. For the free recall task, the model integrates concepts from retrieved context theory, including initiation of recall based on the content of a context cue. With these mechanisms, the model is able to account for serial position and temporal contiguity effects using novel mechanisms not directly instantiated in the variants of CMR explored here. Similarities in predictions offered by the different models indicate that they include analogous features, but important explanatory differences may just as well exist between them and MINERVA-based instance models under certain research conditions as exist between prototype-based and instance-based models in others. Deeper clarification of the distinctions and homologies between different models characterizing performance on memory tasks such as free recall is critical for driving further modeling innovation.
