## Instance and Prototype Accounts of Abstraction

A central task of memory is to relate features of current experience with relevant and useful information from past experience; however, stored information relevant to a probe is often distributed across multiple learning episodes.
To account for our ability to retrieve this information, models of memory search often specify some mechanism for abstraction -- selective generalization across recurrent features of past experience [@yee2019abstraction].
Abstraction involves identifying and highlighting features common across experiential episodes while disregarding or suppressing reinstatement more idiosyncratic properties.
Since this capacity is central to how memory systems retrieve relevant information from stored experience, much work has explored how humans carry it out.

Depending on how they characterize the process of abstraction, memory models can often be categorized as prototype- or instance-based.
Prototype-based models conceptualize abstraction as a process enacted during encoding; new experiences are conceived as updating memory representations to reflect prototypical features that are common across past experiences.
Connectionist models such as the multilayer perceptron are typically examples of prototype-based models [@jamieson2018instance].
Instead of being stored as separate records in memory, learning examples presented to a connectionist model each update a prototypical pattern of weights that eventually map memory probes to responses.

Instance-based models do store learning examples as separate records in memory.
The model architecture was originally identified to help understand how category learning might be possible without explicit storage of so-called abstract ideas [@hintzman1984minerva; @hintzman1986schema; @hintzman1988judgments].
Instance-based models posit that memory encoding primarily involves accumulating a record of every experience as separate traces in memory.
Abstraction over stored instances later occurs at retrieval rather than during encoding, and unfolds through comparison of a memory cue with each instance stored in memory.
The abstractive representation finally retrieved is a blend of the content in each stored instance, weighted such that information in the instances most similar to the probe is substantially more prominent than information in instances that are dissimilar to the probe.
Because instance-based models preserve a discrete record of all relevant events in memory, they can often selectively retrieve information about even rare past events with high flexibility.
<!--
Instance-based architecture might predate minerva 2??
-->

Instance-based accounts of memory have faced scrutiny for implying that the number of stored instances in memory can increase without limit and are all contacted upon retrieval, respectively placing extraordinary capacity and processing demands on the human nervous system [e.g., @kahana2020computational]. However, at the same time as instance-based models have been critiqued for their architectural lack of data compression at storage, the way abstractive representations exclude idiosyncratic features of individual learning episodes to reflect a center of tendency across them is similarly recurrently cited as a limitation of prototype-based models. In research on categorization for example, 'exemplar-similarity' models [@nosofsky2002exemplar; @stanton2002comparisons] outperform comparable prototype-based models by representing categories as sets of stored instances paired with a process for comparison against probes. A related analysis extends these findings to also critique prototype-based accounts of semantic memory. @jamieson2018instance found that because prototype-based distributional semantic models such as latent semantic analysis [@dumais2004latent] and Word2Vec [@church2017word2vec] "collapse the many contexts in which a word occurs to a single best-fitting representation", they lose the ability to represent rare senses of homonymous and polymsemous words. Consequently, prototype-based models exhibited measureably worse performance accounting for word similarity patterns in various natural language simulations compared to an instance-based account of semantic memory based on the MINERVA 2 multiple traces memory model [@hintzman1984minerva]. In the context of successes like these across diverse research conditions, instance-based accounts of memory have become increasingly prominent.

### Models of Free Recall are Traditionally Prototype-Based

While instance-based models have organized formal work in a variety of research subfields, models of memory search primarily focused on accounting for performance on the free recall task largely countervail this pattern. In the free recall task paradigm, research participants are presented a sequence of items --- usually a word list --- to memorize during a study phase. Later, after a delay or perhaps some distraction task, participants are prompted to recall as many items from the list as possible, in whatever order they come to mind. Since participants largely organize the course of retrieval themselves in the response phase of a free recall task, work by researchers to characterize the organization of responses measured under the paradigm [@postman1971organization; @puff1979memory] have provided important constraints on accounts of the representations and mechanisms underlying search through memory to retrieve information.

In particular, three consistent regularities across experiments have received especial emphasis in accounts of performance on the free recall task [@kahana2020computational]. The serial position effect identifies a nonlinear, U-shaped relationship between the position of an item within a study list --- its serial position --- and its probability of retrieval after encoding [@murdock1962serial]. Researchers typically distinguish between the enhanced retrieval probabilities for early and terminal items; the advantage for the initially presented items is called the primacy effect, while the normally larger advantage for the few presented items is called the recency effect.

A similar but distinct pattern constraining accounts of memory search is found in analyses relating an item's serial position with the probability that it will be recalled first in the retrieval phase of experiments. Pivotally, in list-based free recall tasks, participants tend to initiate recall with the most recently studied items from the list; however, in a *serial* recall task where participants are instructed to recall items in the order in which they were presented rather than freely, participants tend to successfully recall the earliest presented items first [for example in @golomb2008effects]. This difference implies that while participants maintain and can access memories of item positions to perform a serial recall task, memory search and retrieval is organized by other features of experience.

Primacy and recency effects demonstrate that the temporal structure of the list affects the memorability of the items within it. This temporal structure can also be seen in the organization of responses throughout the response sequence, not just for initial and terminal items or recall positions. Free recall task data exhibits a pattern called *temporal contiguity* where items studied at nearby serial positions tend to be recalled near one another at the retrieval phase of an experiment. To quantify this pattern, researchers measure across trials the conditional probability of retrieving items given increasing inter-item lags between the serial positions of considered items and the serial position of the item last recalled. These lag-based condition response probability (lag-CRP) analyses find that subjects reliably tend to make transitions between temporally contiguous items (that is, items presented near one another) during free recall. Furthermore, they exhibit a forward bias, recalling contiguous items presented after the last recalled item more frequently than items presented before [@kahana1996associative].

To account for these phenomena, the formal literature has largely converged on retrieved context theories of memory search [for example, @howard2002distributed; @polyn2009context; @morton2016predictive]. Generally, according to these theories, as items are encoded into a memory system, an internal representational of temporal context is also maintained that dynamically updates itself to reflect a weighted summary of recent experience. As each item is studied, a Hebbian learning mechanism associates the item's features to the current state of the context representation. Once associated, item features can cue retrieval of associated contextual features, and vice versa. When the retrieval phase comes, the current contextual representation can drive memory search by activating a blend of associated item features. This prompts a retrieval competition in which a particular item is selected and retrieved. Correspondingly, retrieving an item reactivates its associated contextual features, updating context before the next recall attempt. The retrieved context supports the neighbors of the just-recalled item, which gives rise to temporal organization.

With these basic mechanisms, retrieved-context models have been used to explain many phenomena, including serial and temporal organizational effects in list-learning tasks [@polyn2009context; @siegel2014retrieved; @schwartz2005shadows], and broader domains such as financial decision making [@wachter2019retrieved], emotion regulation [@talmi2019retrieved], and neural signal dynamics within the medial temporal lobe [@kragel2015neural]. Further model development has integrated retrieved context accounts of memory search with theories of semantic knowledge [@morton2016predictive] and changes related to healthy aging [@healey2016four].

The framework used to implement most retrieved context models of memory search acts like a prototype model. These models typically encode memories associating contextual states and item features by updating connection weights within a simplified neural network. Through Hebbian learning, where co-activation of item and contextual features increase weights associating those features, the network accumulates a collapsed average representation reflecting the history of context and item interactions across experience. During retrieval, the network can be probed with a contextual cue to retrieve an item feature representation (or vice versa) based on a linear function of the cue's content and stored context-to-item weights.

In contrast, an instance-based alternative would track this history by storing a discrete record of each experience with its unique temporal context in memory to perform abstraction over only at the point of retrieval. Previous instance-based accounts of performance on various tasks have emphasized a role of some sort of temporal contextual representation in organizing performance. Indeed, the original presentation of MINERVA 2, the first major instance-based memory modeling architecture, included a representation of list context as a feature in stored memory instances to model source-specific frequency judgments from memory [@hintzman1984minerva]. [@lehman2013buffer] proposed an instance-based buffer model that accounts for patterns like recency and the position position effect in terms of storage and retrieval of traces containing information about item and contextual co-occurrences. Most recently, @logan2021serial introduced the Context Retrieval and Updating (CRU) model, which extends retrieved context theories' conceptualization of context as a recency-weighted history of previously presented items to account for performance on whole report, serial recall, and copy typing tasks. Nonetheless, it remains unclear whether differences reported in related memory literatures between the performance of prototype- and instance-based memory models might similarly distinguish models of memory search.

### Research Approach

In this paper, I show that the mechanisms proposed by the influential Context Maintanence and Retrieval (CMR) model of memory search [@morton2016predictive] can be realized within either a prototypical or instance-based model architecture without substantially impacting performance across various experimental conditions. This instance-based CMR (InstanceCMR) extends the established MINERVA 2 multiple traces model [@hintzman1984minerva; @hintzman1986schema; @hintzman1988judgments] to support context-based memory search and simulate performance on the free recall task. I fit InstanceCMR and its original prototype-based counterpart (prototypeCMR) to the sequences of individual responses made by participants in three distinct free recall task datasets.I find that the models account for retrieval performance with similar effectiveness despite architectural differences, including over data manipulating the lengths of study lists between trials and other data manipulating the number of times particular items are studied within trials.

Analyses of the two specifications for CMR suggest that these outcomes can be largely explained by the model's assumption that feature representations corresponding to studied items in free recall experiments are orthogonal --- activation of each unit on an item feature layer corresponds to one item. This ensures that context-to-feature associations built via experience of one item do not overlap with associations built through experience of some other distinct item. Correspondingly, the influence of relevant experiences on the content of abstractive representations retrieved via these associations can be selectively enhanced while simultaneously suppressing the influence of less relevant experiences, without any interference. This capacity to nonlinearly modulate the influence of selected learning episodes on recall based on the content of a probe approximates trace-based activation functions realized within instance-based models, sidestepping issues reported about prototype-based memory models in other literatures.
